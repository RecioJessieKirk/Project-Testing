{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Train dataset...\n",
      "Copying 5600 images for Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5600/5600 [01:05<00:00, 85.90img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Validation dataset...\n",
      "Copying 1200 images for Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:06<00:00, 175.06img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:11<00:00, 100.78img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying annotations...\n",
      "\n",
      "Subset creation complete! (~2GB total)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm  # Import progress bar\n",
    "\n",
    "# Define original dataset paths\n",
    "original_base_path = \"coco2017\"\n",
    "original_train_images_path = os.path.join(original_base_path, \"train2017\")\n",
    "original_val_images_path = os.path.join(original_base_path, \"val2017\")\n",
    "original_test_images_path = os.path.join(original_base_path, \"test2017\")\n",
    "original_annotations_path = os.path.join(original_base_path, \"annotations\")\n",
    "\n",
    "# Define subset paths\n",
    "subset_base_path = \"cocosubset\"\n",
    "subset_train_images_path = os.path.join(subset_base_path, \"train2017\")\n",
    "subset_val_images_path = os.path.join(subset_base_path, \"val2017\")\n",
    "subset_test_images_path = os.path.join(subset_base_path, \"test2017\")\n",
    "subset_annotations_path = os.path.join(subset_base_path, \"annotations\")\n",
    "\n",
    "# Define annotation file paths\n",
    "original_train_annotations_path = os.path.join(original_annotations_path, \"instances_train2017.json\")\n",
    "original_val_annotations_path = os.path.join(original_annotations_path, \"instances_val2017.json\")\n",
    "\n",
    "subset_train_annotations_path = os.path.join(subset_annotations_path, \"instances_train2017.json\")\n",
    "subset_val_annotations_path = os.path.join(subset_annotations_path, \"instances_val2017.json\")\n",
    "\n",
    "# Set image counts based on ~2GB total size\n",
    "train_subset_size = 5600  # ~1.4GB\n",
    "val_subset_size = 1200    # ~0.3GB\n",
    "test_subset_size = 1200   # ~0.3GB\n",
    "\n",
    "# Function to create image subset and annotations\n",
    "def create_subset(original_images_path, original_annotations_path, subset_images_path, subset_annotations_path, subset_size, dataset_name):\n",
    "    print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "\n",
    "    # Load COCO annotations\n",
    "    with open(original_annotations_path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Get a random subset of images\n",
    "    all_images = coco_data[\"images\"]\n",
    "    subset_images = random.sample(all_images, subset_size)\n",
    "\n",
    "    # Create new annotation file\n",
    "    subset_annotations = {\n",
    "        \"info\": coco_data[\"info\"],\n",
    "        \"licenses\": coco_data[\"licenses\"],\n",
    "        \"images\": subset_images,\n",
    "        \"annotations\": [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] in {img[\"id\"] for img in subset_images}],\n",
    "        \"categories\": coco_data[\"categories\"],\n",
    "    }\n",
    "\n",
    "    # Save new annotations file\n",
    "    os.makedirs(os.path.dirname(subset_annotations_path), exist_ok=True)\n",
    "    with open(subset_annotations_path, \"w\") as f:\n",
    "        json.dump(subset_annotations, f, indent=4)\n",
    "\n",
    "    # Copy selected images with progress bar\n",
    "    os.makedirs(subset_images_path, exist_ok=True)\n",
    "    print(f\"Copying {subset_size} images for {dataset_name}...\")\n",
    "    for img in tqdm(subset_images, desc=f\"Copying {dataset_name}\", unit=\"img\"):\n",
    "        src = os.path.join(original_images_path, img[\"file_name\"])\n",
    "        dst = os.path.join(subset_images_path, img[\"file_name\"])\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# Create train subset\n",
    "create_subset(original_train_images_path, original_train_annotations_path, subset_train_images_path, subset_train_annotations_path, train_subset_size, \"Train\")\n",
    "\n",
    "# Create validation subset\n",
    "create_subset(original_val_images_path, original_val_annotations_path, subset_val_images_path, subset_val_annotations_path, val_subset_size, \"Validation\")\n",
    "\n",
    "# Create test subset (random selection if no annotations)\n",
    "if os.path.exists(original_test_images_path):\n",
    "    os.makedirs(subset_test_images_path, exist_ok=True)\n",
    "    test_images = random.sample(os.listdir(original_test_images_path), test_subset_size)\n",
    "    print(\"\\nCopying test images...\")\n",
    "    for img in tqdm(test_images, desc=\"Copying Test\", unit=\"img\"):\n",
    "        shutil.copy(os.path.join(original_test_images_path, img), os.path.join(subset_test_images_path, img))\n",
    "\n",
    "# Copy full annotations folder (for compatibility)\n",
    "print(\"\\nCopying annotations...\")\n",
    "shutil.copytree(original_annotations_path, subset_annotations_path, dirs_exist_ok=True)\n",
    "\n",
    "print(\"\\nSubset creation complete! (~2GB total)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train Annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 860001/860001 [00:18<00:00, 45430.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… train dataset conversion complete!\n",
      "\n",
      "ðŸ”„ Processing val dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val Annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36781/36781 [00:02<00:00, 17127.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… val dataset conversion complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd  # Using pandas instead of cudf\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths (Update these if necessary)\n",
    "annotations_paths = {\n",
    "    \"train\": (\"cocosubset/annotations/instances_train2017.json\", \"cocosubset/train2017\", \"cocosubset/labels/train2017\"),\n",
    "    \"val\": (\"cocosubset/annotations/instances_val2017.json\", \"cocosubset/val2017\", \"cocosubset/labels/val2017\"),\n",
    "}\n",
    "\n",
    "# Process both train and val datasets\n",
    "for dataset, (coco_json, image_folder, output_label_folder) in annotations_paths.items():\n",
    "    print(f\"\\nðŸ”„ Processing {dataset} dataset...\")\n",
    "\n",
    "    # Create labels folder if it doesn't exist\n",
    "    os.makedirs(output_label_folder, exist_ok=True)\n",
    "\n",
    "    # Load COCO JSON\n",
    "    with open(coco_json, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Get image filenames present in the folder\n",
    "    existing_images = set(os.listdir(image_folder))\n",
    "\n",
    "    # Create a dictionary for fast lookups (image_id â†’ (width, height, filename))\n",
    "    image_id_to_data = {\n",
    "        img[\"id\"]: (img[\"width\"], img[\"height\"], img[\"file_name\"])\n",
    "        for img in coco_data[\"images\"] if img[\"file_name\"] in existing_images\n",
    "    }\n",
    "\n",
    "    # Map COCO category IDs to YOLO format category IDs\n",
    "    category_map = {cat[\"id\"]: idx for idx, cat in enumerate(coco_data[\"categories\"])}\n",
    "\n",
    "    # Convert annotations into a pandas DataFrame\n",
    "    annotations_df = pd.DataFrame.from_records(coco_data[\"annotations\"])\n",
    "\n",
    "    # Function to process a single annotation\n",
    "    def process_annotation(ann):\n",
    "        image_id = ann[\"image_id\"]\n",
    "\n",
    "        # Ensure image_id exists\n",
    "        if image_id in image_id_to_data:\n",
    "            img_width, img_height, file_name = image_id_to_data[image_id]\n",
    "            txt_file = os.path.join(output_label_folder, file_name.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "            # Extract bbox and convert to YOLO format\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            x_center = (x + w / 2) / img_width\n",
    "            y_center = (y + h / 2) / img_height\n",
    "            w_norm = w / img_width\n",
    "            h_norm = h / img_height\n",
    "            category = category_map[ann[\"category_id\"]]\n",
    "\n",
    "            # Save to .txt file\n",
    "            with open(txt_file, \"a\") as f:\n",
    "                f.write(f\"{category} {x_center} {y_center} {w_norm} {h_norm}\\n\")\n",
    "\n",
    "    # Process annotations using multiple threads for speed\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        list(tqdm(executor.map(process_annotation, annotations_df.to_dict(orient=\"records\")), \n",
    "                  total=len(annotations_df), desc=f\"Processing {dataset} Annotations\"))\n",
    "\n",
    "    print(f\"âœ… {dataset} dataset conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIND AND FIX INCORRECT LABELS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '0.041015625'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Check for invalid class IDs\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m valid_lines \u001b[38;5;241m=\u001b[39m [line \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m80\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Overwrite file if changes were made\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_lines) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines):\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '0.041015625'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "labels_path = r\"C:\\Users\\Kirk Recio\\Documents\\PYTHON\\Project Testing\\Data Cleaning Training\\cocosubset\\labels\"\n",
    "\n",
    "for split in [\"train2017\", \"val2017\"]:\n",
    "    split_path = os.path.join(labels_path, split)\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(split_path):\n",
    "        print(f\"ðŸš¨ Folder not found: {split_path}\")\n",
    "        continue  # Skip to the next loop iteration\n",
    "\n",
    "    for label_file in os.listdir(split_path):\n",
    "        with open(os.path.join(split_path, label_file), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Check for invalid class IDs\n",
    "        valid_lines = [line for line in lines if int(line.split()[0]) < 80]\n",
    "\n",
    "        # Overwrite file if changes were made\n",
    "        if len(valid_lines) != len(lines):\n",
    "            with open(os.path.join(split_path, label_file), \"w\") as f:\n",
    "                f.writelines(valid_lines)\n",
    "            print(f\"âœ… Fixed {label_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid labels removed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "labels_dir = r\"cocosubset\\labels\\train2017\"  # Use raw string or forward slashes\n",
    "max_classes = 80  # Expected max class index\n",
    "\n",
    "for file in os.listdir(labels_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        path = os.path.join(labels_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        valid_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) > 0 and parts[0].isdigit():\n",
    "                class_id = int(parts[0])\n",
    "                if class_id < max_classes:\n",
    "                    valid_lines.append(line)\n",
    "\n",
    "        if valid_lines:  # Only write if there are valid labels\n",
    "            with open(path, \"w\") as f:\n",
    "                f.writelines(valid_lines)\n",
    "        else:\n",
    "            os.remove(path)  # Remove empty label file\n",
    "\n",
    "print(\"Invalid labels removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POTHOLE DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Processing train2017 set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train2017:   0%|          | 0/465 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train2017: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:03<00:00, 121.77file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Processing val2017 set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting val2017: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:00<00:00, 148.46file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Processing test2017 set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting test2017: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:00<00:00, 154.41file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset conversion and organization completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Root dataset folder\n",
    "DATASET_PATH = \"pothole\"\n",
    "\n",
    "# Paths to annotations and images inside \"pothole\"\n",
    "ANNOTATIONS_PATH = os.path.join(DATASET_PATH, \"annotations\")\n",
    "IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "\n",
    "# YOLO formatted dataset structure (train, val, test)\n",
    "YOLO_IMAGES_TRAIN = os.path.join(DATASET_PATH, \"images/train2017\")\n",
    "YOLO_IMAGES_VAL = os.path.join(DATASET_PATH, \"images/val2017\")\n",
    "YOLO_IMAGES_TEST = os.path.join(DATASET_PATH, \"images/test2017\")\n",
    "YOLO_LABELS_TRAIN = os.path.join(DATASET_PATH, \"labels/train2017\")\n",
    "YOLO_LABELS_VAL = os.path.join(DATASET_PATH, \"labels/val2017\")\n",
    "YOLO_LABELS_TEST = os.path.join(DATASET_PATH, \"labels/test2017\")  # âœ… Now included!\n",
    "\n",
    "# Ensure YOLO directories exist\n",
    "for path in [YOLO_IMAGES_TRAIN, YOLO_IMAGES_VAL, YOLO_IMAGES_TEST, \n",
    "             YOLO_LABELS_TRAIN, YOLO_LABELS_VAL, YOLO_LABELS_TEST]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Class names (modify if needed)\n",
    "classes = [\"pothole\"]\n",
    "\n",
    "def convert_voc_to_yolo(xml_file):\n",
    "    \"\"\" Convert Pascal VOC XML annotations to YOLO format \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_width = int(root.find(\"size/width\").text)\n",
    "    image_height = int(root.find(\"size/height\").text)\n",
    "\n",
    "    yolo_annotations = []\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        class_name = obj.find(\"name\").text\n",
    "        if class_name not in classes:\n",
    "            continue\n",
    "        class_id = classes.index(class_name)\n",
    "\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = int(bbox.find(\"xmin\").text)\n",
    "        ymin = int(bbox.find(\"ymin\").text)\n",
    "        xmax = int(bbox.find(\"xmax\").text)\n",
    "        ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "        # Convert to YOLO format\n",
    "        x_center = (xmin + xmax) / 2 / image_width\n",
    "        y_center = (ymin + ymax) / 2 / image_height\n",
    "        width = (xmax - xmin) / image_width\n",
    "        height = (ymax - ymin) / image_height\n",
    "\n",
    "        yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "\n",
    "    return image_name, yolo_annotations\n",
    "\n",
    "# Ensure annotations directory exists\n",
    "if not os.path.exists(ANNOTATIONS_PATH):\n",
    "    raise FileNotFoundError(f\"Annotations folder '{ANNOTATIONS_PATH}' not found!\")\n",
    "\n",
    "# Get all XML annotation files\n",
    "all_xml_files = [f for f in os.listdir(ANNOTATIONS_PATH) if f.endswith(\".xml\")]\n",
    "\n",
    "# Shuffle dataset and split into train (70%), val (20%), test (10%)\n",
    "random.shuffle(all_xml_files)\n",
    "train_split = int(0.7 * len(all_xml_files))\n",
    "val_split = int(0.9 * len(all_xml_files))  # 70% train + 20% val = 90%, remaining 10% is test\n",
    "\n",
    "train_files = all_xml_files[:train_split]\n",
    "val_files = all_xml_files[train_split:val_split]\n",
    "test_files = all_xml_files[val_split:]\n",
    "\n",
    "# Process files with progress bar\n",
    "for dataset_type, xml_files, img_dest, label_dest in [\n",
    "    (\"train2017\", train_files, YOLO_IMAGES_TRAIN, YOLO_LABELS_TRAIN),\n",
    "    (\"val2017\", val_files, YOLO_IMAGES_VAL, YOLO_LABELS_VAL),\n",
    "    (\"test2017\", test_files, YOLO_IMAGES_TEST, YOLO_LABELS_TEST)  # âœ… Now processes test2017\n",
    "]:\n",
    "    print(f\"ðŸ“‚ Processing {dataset_type} set...\")\n",
    "    \n",
    "    for xml_file in tqdm(xml_files, desc=f\"Converting {dataset_type}\", unit=\"file\"):\n",
    "        xml_path = os.path.join(ANNOTATIONS_PATH, xml_file)\n",
    "        image_name, yolo_annotations = convert_voc_to_yolo(xml_path)\n",
    "\n",
    "        # Save YOLO annotation file\n",
    "        yolo_label_path = os.path.join(label_dest, image_name.replace(\".png\", \".txt\"))\n",
    "        with open(yolo_label_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_annotations))\n",
    "\n",
    "        # Move corresponding image\n",
    "        src_img_path = os.path.join(IMAGES_PATH, image_name)\n",
    "        dst_img_path = os.path.join(img_dest, image_name)\n",
    "        if os.path.exists(src_img_path):\n",
    "            shutil.copy(src_img_path, dst_img_path)\n",
    "\n",
    "print(\"âœ… Dataset conversion and organization completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EASY-OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICDAR 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renaming files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34282/34282 [00:36<00:00, 930.17file/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Renaming completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the dataset path\n",
    "DATASET_PATH = \"icdar2015/data\"  # Change this if needed\n",
    "\n",
    "# Get all files\n",
    "files = os.listdir(DATASET_PATH)\n",
    "\n",
    "# Rename all .jpg and .txt files\n",
    "for filename in tqdm(files, desc=\"Renaming files\", unit=\"file\"):\n",
    "    old_path = os.path.join(DATASET_PATH, filename)\n",
    "    \n",
    "    # Check if the file is a .jpg or .txt\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".txt\"):\n",
    "        new_filename = f\"letter{filename}\"\n",
    "        new_path = os.path.join(DATASET_PATH, new_filename)\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "print(\"âœ… Renaming completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Remove Corrupted Images & Empty Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34282/34282 [03:16<00:00, 174.79file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Image cleaning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking annotations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34282/34282 [00:00<00:00, 41061.35file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Annotation cleaning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# Get list of files\n",
    "DATASET_PATH = \"icdar2015/data\"  # Change this if needed\n",
    "files = os.listdir(DATASET_PATH)\n",
    "\n",
    "# Remove corrupted images\n",
    "for filename in tqdm(files, desc=\"Checking images\", unit=\"file\"):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(DATASET_PATH, filename)\n",
    "        \n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"âŒ Corrupted image: {filename} (Deleting...)\")\n",
    "                os.remove(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "            os.remove(img_path)\n",
    "\n",
    "print(\"âœ… Image cleaning completed!\")\n",
    "\n",
    "# Remove empty annotation files\n",
    "for filename in tqdm(files, desc=\"Checking annotations\", unit=\"file\"):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        txt_path = os.path.join(DATASET_PATH, filename)\n",
    "        \n",
    "        if os.path.getsize(txt_path) == 0:\n",
    "            print(f\"âŒ Empty annotation file: {filename} (Deleting...)\")\n",
    "            os.remove(txt_path)\n",
    "\n",
    "print(\"âœ… Annotation cleaning completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train.txt and val.txt with Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating train.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13712/13712 [00:00<00:00, 3984501.62line/s]\n",
      "Updating val.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3429/3429 [00:00<00:00, 3109679.66line/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… train.txt and val.txt updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the train and val lists\n",
    "with open(\"icdar2015/train.txt\", \"r\") as f:\n",
    "    train_files = f.readlines()\n",
    "with open(\"icdar2015/val.txt\", \"r\") as f:\n",
    "    val_files = f.readlines()\n",
    "\n",
    "# Rename inside the files with progress\n",
    "train_files = [f\"letter{line.strip()}\\n\" for line in tqdm(train_files, desc=\"Updating train.txt\", unit=\"line\")]\n",
    "val_files = [f\"letter{line.strip()}\\n\" for line in tqdm(val_files, desc=\"Updating val.txt\", unit=\"line\")]\n",
    "\n",
    "# Write back the updated lists\n",
    "with open(\"icdar2015/train.txt\", \"w\") as f:\n",
    "    f.writelines(train_files)\n",
    "with open(\"icdar2015/val.txt\", \"w\") as f:\n",
    "    f.writelines(val_files)\n",
    "\n",
    "print(\"âœ… train.txt and val.txt updated!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
