{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Train dataset...\n",
      "Copying 5600 images for Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Train: 100%|██████████| 5600/5600 [01:05<00:00, 85.90img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Validation dataset...\n",
      "Copying 1200 images for Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Validation: 100%|██████████| 1200/1200 [00:06<00:00, 175.06img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying Test: 100%|██████████| 1200/1200 [00:11<00:00, 100.78img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying annotations...\n",
      "\n",
      "Subset creation complete! (~2GB total)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm  # Import progress bar\n",
    "\n",
    "# Define original dataset paths\n",
    "original_base_path = \"coco2017\"\n",
    "original_train_images_path = os.path.join(original_base_path, \"train2017\")\n",
    "original_val_images_path = os.path.join(original_base_path, \"val2017\")\n",
    "original_test_images_path = os.path.join(original_base_path, \"test2017\")\n",
    "original_annotations_path = os.path.join(original_base_path, \"annotations\")\n",
    "\n",
    "# Define subset paths\n",
    "subset_base_path = \"cocosubset\"\n",
    "subset_train_images_path = os.path.join(subset_base_path, \"train2017\")\n",
    "subset_val_images_path = os.path.join(subset_base_path, \"val2017\")\n",
    "subset_test_images_path = os.path.join(subset_base_path, \"test2017\")\n",
    "subset_annotations_path = os.path.join(subset_base_path, \"annotations\")\n",
    "\n",
    "# Define annotation file paths\n",
    "original_train_annotations_path = os.path.join(original_annotations_path, \"instances_train2017.json\")\n",
    "original_val_annotations_path = os.path.join(original_annotations_path, \"instances_val2017.json\")\n",
    "\n",
    "subset_train_annotations_path = os.path.join(subset_annotations_path, \"instances_train2017.json\")\n",
    "subset_val_annotations_path = os.path.join(subset_annotations_path, \"instances_val2017.json\")\n",
    "\n",
    "# Set image counts based on ~2GB total size\n",
    "train_subset_size = 5600  # ~1.4GB\n",
    "val_subset_size = 1200    # ~0.3GB\n",
    "test_subset_size = 1200   # ~0.3GB\n",
    "\n",
    "# Function to create image subset and annotations\n",
    "def create_subset(original_images_path, original_annotations_path, subset_images_path, subset_annotations_path, subset_size, dataset_name):\n",
    "    print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "\n",
    "    # Load COCO annotations\n",
    "    with open(original_annotations_path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Get a random subset of images\n",
    "    all_images = coco_data[\"images\"]\n",
    "    subset_images = random.sample(all_images, subset_size)\n",
    "\n",
    "    # Create new annotation file\n",
    "    subset_annotations = {\n",
    "        \"info\": coco_data[\"info\"],\n",
    "        \"licenses\": coco_data[\"licenses\"],\n",
    "        \"images\": subset_images,\n",
    "        \"annotations\": [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] in {img[\"id\"] for img in subset_images}],\n",
    "        \"categories\": coco_data[\"categories\"],\n",
    "    }\n",
    "\n",
    "    # Save new annotations file\n",
    "    os.makedirs(os.path.dirname(subset_annotations_path), exist_ok=True)\n",
    "    with open(subset_annotations_path, \"w\") as f:\n",
    "        json.dump(subset_annotations, f, indent=4)\n",
    "\n",
    "    # Copy selected images with progress bar\n",
    "    os.makedirs(subset_images_path, exist_ok=True)\n",
    "    print(f\"Copying {subset_size} images for {dataset_name}...\")\n",
    "    for img in tqdm(subset_images, desc=f\"Copying {dataset_name}\", unit=\"img\"):\n",
    "        src = os.path.join(original_images_path, img[\"file_name\"])\n",
    "        dst = os.path.join(subset_images_path, img[\"file_name\"])\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# Create train subset\n",
    "create_subset(original_train_images_path, original_train_annotations_path, subset_train_images_path, subset_train_annotations_path, train_subset_size, \"Train\")\n",
    "\n",
    "# Create validation subset\n",
    "create_subset(original_val_images_path, original_val_annotations_path, subset_val_images_path, subset_val_annotations_path, val_subset_size, \"Validation\")\n",
    "\n",
    "# Create test subset (random selection if no annotations)\n",
    "if os.path.exists(original_test_images_path):\n",
    "    os.makedirs(subset_test_images_path, exist_ok=True)\n",
    "    test_images = random.sample(os.listdir(original_test_images_path), test_subset_size)\n",
    "    print(\"\\nCopying test images...\")\n",
    "    for img in tqdm(test_images, desc=\"Copying Test\", unit=\"img\"):\n",
    "        shutil.copy(os.path.join(original_test_images_path, img), os.path.join(subset_test_images_path, img))\n",
    "\n",
    "# Copy full annotations folder (for compatibility)\n",
    "print(\"\\nCopying annotations...\")\n",
    "shutil.copytree(original_annotations_path, subset_annotations_path, dirs_exist_ok=True)\n",
    "\n",
    "print(\"\\nSubset creation complete! (~2GB total)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
